{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09687ef4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-26T15:42:21.404262Z",
     "iopub.status.busy": "2025-12-26T15:42:21.404025Z",
     "iopub.status.idle": "2025-12-26T16:56:50.826202Z",
     "shell.execute_reply": "2025-12-26T16:56:50.825112Z"
    },
    "papermill": {
     "duration": 4469.426599,
     "end_time": "2025-12-26T16:56:50.828030",
     "exception": false,
     "start_time": "2025-12-26T15:42:21.401431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 15:42:38.831011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766763759.024817      23 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766763759.083110      23 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766763759.555335      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766763759.555372      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766763759.555375      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766763759.555377      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /kaggle/input/phobert-ckpt2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.1839\n",
      "Epoch 2/10 - Loss: 0.0907\n",
      "Epoch 3/10 - Loss: 0.0543\n",
      "Epoch 4/10 - Loss: 0.0413\n",
      "Epoch 5/10 - Loss: 0.0305\n",
      "Epoch 6/10 - Loss: 0.0206\n",
      "Epoch 7/10 - Loss: 0.0335\n",
      "Epoch 8/10 - Loss: 0.0183\n",
      "Epoch 9/10 - Loss: 0.0132\n",
      "Epoch 10/10 - Loss: 0.0138\n",
      "\n",
      "===== VALIDATION METRICS =====\n",
      "Accuracy          : 0.9709\n",
      "F1 (macro)        : 0.9651\n",
      "F1 (weighted)     : 0.9706\n",
      "Precision (macro) : 0.9776\n",
      "Recall (macro)    : 0.9547\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9622    0.9971    0.9793       689\n",
      "           1     0.9929    0.9123    0.9509       308\n",
      "\n",
      "    accuracy                         0.9709       997\n",
      "   macro avg     0.9776    0.9547    0.9651       997\n",
      "weighted avg     0.9717    0.9709    0.9706       997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# IMPORT\n",
    "# =========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "import shutil\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PHOBERT_DIR = \"/kaggle/input/phobert-ckpt2\"  # PhoBERT pretrained\n",
    "TRAIN_FILE = \"/kaggle/input/nlp-final-prj2/train_data1.csv\"\n",
    "VAL_FILE = \"/kaggle/input/nlp-final-prj2/val_data1.csv\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 256\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.3\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "df_train = pd.read_csv(TRAIN_FILE)\n",
    "df_val = pd.read_csv(VAL_FILE)\n",
    "\n",
    "train_texts = df_train[TEXT_COL].astype(str).tolist()\n",
    "train_labels = df_train[LABEL_COL].astype(int).to_numpy()\n",
    "\n",
    "val_texts = df_val[TEXT_COL].astype(str).tolist()\n",
    "val_labels = df_val[LABEL_COL].astype(int).to_numpy()\n",
    "\n",
    "# =========================\n",
    "# TOKENIZER & PHOBERT\n",
    "# =========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(PHOBERT_DIR, local_files_only=True)\n",
    "phobert_model = AutoModel.from_pretrained(PHOBERT_DIR, local_files_only=True)\n",
    "phobert_model.to(DEVICE)\n",
    "phobert_model.train()  # allow gradients\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # squeeze to remove batch dim\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item, label\n",
    "\n",
    "train_dataset = NewsDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = NewsDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# =========================\n",
    "# LSTM CLASSIFIER\n",
    "# =========================\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, phobert, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=DROPOUT, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.phobert = phobert\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=phobert.config.hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers>1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # embeddings: [batch, seq_len, hidden]\n",
    "        x = outputs.last_hidden_state\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = out[:, -1, :]  # last time step\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "model = LSTMClassifier(phobert_model)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# =========================\n",
    "# LOSS & OPTIMIZER\n",
    "# =========================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# =========================\n",
    "# TRAIN LOOP\n",
    "# =========================\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, label in train_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# =========================\n",
    "# EVALUATION\n",
    "# =========================\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, label in val_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        \n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "f1_weighted = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "precision_macro = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "recall_macro = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "print(\"\\n===== VALIDATION METRICS =====\")\n",
    "print(f\"Accuracy          : {acc:.4f}\")\n",
    "print(f\"F1 (macro)        : {f1_macro:.4f}\")\n",
    "print(f\"F1 (weighted)     : {f1_weighted:.4f}\")\n",
    "print(f\"Precision (macro) : {precision_macro:.4f}\")\n",
    "print(f\"Recall (macro)    : {recall_macro:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225acaf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:56:50.833792Z",
     "iopub.status.busy": "2025-12-26T16:56:50.832917Z",
     "iopub.status.idle": "2025-12-26T16:57:26.875562Z",
     "shell.execute_reply": "2025-12-26T16:57:26.874774Z"
    },
    "papermill": {
     "duration": 36.04758,
     "end_time": "2025-12-26T16:57:26.877749",
     "exception": false,
     "start_time": "2025-12-26T16:56:50.830169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: /kaggle/working/phobert_lstm_finetuned.zip\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = \"/kaggle/working/phobert_lstm_finetuned\"\n",
    "ZIP_PATH = \"/kaggle/working/phobert_lstm_finetuned.zip\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Lưu state_dict\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"model.pt\"))\n",
    "\n",
    "# Lưu config cần thiết để load lại\n",
    "save_config = {\n",
    "    \"hidden_size\": HIDDEN_SIZE,\n",
    "    \"num_layers\": NUM_LAYERS,\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"num_classes\": 2\n",
    "}\n",
    "torch.save(save_config, os.path.join(SAVE_DIR, \"config.pt\"))\n",
    "\n",
    "# Zip toàn bộ thư mục\n",
    "shutil.make_archive(\n",
    "    base_name=ZIP_PATH.replace(\".zip\", \"\"),\n",
    "    format=\"zip\",\n",
    "    root_dir=SAVE_DIR\n",
    ")\n",
    "\n",
    "print(\"Model saved to:\", ZIP_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9124797,
     "sourceId": 14294603,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9124414,
     "sourceId": 14294059,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4511.485366,
   "end_time": "2025-12-26T16:57:30.394517",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-26T15:42:18.909151",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
